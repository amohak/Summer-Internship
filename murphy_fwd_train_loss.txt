Data Loaded	
nn.Recursor @ nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> output]
  (1): nn.LookupTable
  (2): nn.FastLSTM(299 -> 299)
  (3): nn.Linear(299 -> 2)
  (4): nn.LogSoftMax
}
Epoch 1	
Training Loss : 6789.1744000968	
Epoch 2	
Training Loss : 5287.8190411101	
Epoch 3	
Training Loss : 4789.1432564789	
Epoch 4	
Training Loss : 4423.2259124023	
Epoch 5	
Training Loss : 4085.6795232735	
Epoch 6	
Training Loss : 3766.4524889485	
Epoch 7	
Training Loss : 3470.2540592895	
Epoch 8	
Training Loss : 3251.428334513	
Epoch 9	
Training Loss : 3038.9697999932	
Epoch 10	
Training Loss : 2850.3177033533	
Epoch 11	
Training Loss : 2657.4729571457	
Epoch 12	
Training Loss : 2495.3946054363	
Epoch 13	
Training Loss : 2381.60515272	
Epoch 14	
Training Loss : 2252.071500099	
Epoch 15	
Training Loss : 2315.5830451135	
Epoch 16	
Training Loss : 2068.2542734885	
Epoch 17	
Training Loss : 1888.2289107494	
Epoch 18	
Training Loss : 1759.995044041	
Epoch 19	
Training Loss : 1485.1328045866	
Epoch 20	
Training Loss : 1489.0706712187	
Epoch 21	
Training Loss : 1422.7556724013	
Epoch 22	
Training Loss : 1463.0005415964	
Epoch 23	
Training Loss : 1467.1169816229	
Epoch 24	
Training Loss : 1293.3517276317	
Epoch 25	
Training Loss : 1278.3637299617	
Epoch 26	
Training Loss : 1258.4510172113	
Epoch 27	
Training Loss : 1152.1804745796	
Epoch 28	
Training Loss : 989.22160961012	
Epoch 29	
Training Loss : 844.82756460748	
Epoch 30	
Training Loss : 808.98354571156	
Epoch 31	
Training Loss : 862.21747687121	
Epoch 32	
Training Loss : 768.49427652017	
Epoch 33	
Training Loss : 703.63993877963	
Epoch 34	
Training Loss : 710.08144531857	
Epoch 35	
Training Loss : 631.25198453512	
Epoch 36	
Training Loss : 702.36936348103	
Epoch 37	
Training Loss : 567.08352446911	
Epoch 38	
Training Loss : 515.97337389281	
Epoch 39	
Training Loss : 436.81351394459	
Epoch 40	
Training Loss : 422.96105930087	
Epoch 41	
Training Loss : 578.51482119456	
Epoch 42	
Training Loss : 580.02485846302	
Epoch 43	
Training Loss : 400.7120271904	
Epoch 44	
Training Loss : 372.22410773807	
Epoch 45	
Training Loss : 381.88739832709	
Epoch 46	
Training Loss : 341.66672774305	
Epoch 47	
Training Loss : 299.47673964273	
Epoch 48	
Training Loss : 262.04728816893	
Epoch 49	
Training Loss : 238.15588704001	
Epoch 50	
Training Loss : 224.14634573075	
